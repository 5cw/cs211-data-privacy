{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CS211: Data Privacy\n",
    "## Homework 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0\n",
    "\n",
    "adult = pd.read_csv('https://github.com/jnear/cs211-data-privacy/raw/master/homework/adult_with_pii.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 1 (10 points)\n",
    "\n",
    "(Reference [Chapter 7](https://uvm-plaid.github.io/programming-dp/notebooks/ch7.html) of the textbook)\n",
    "\n",
    "Consider the following minimum query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Cache the sorted ages, because we will use them a lot.\n",
    "age_lower = 0\n",
    "age_upper = 100\n",
    "sorted_ages = adult['Age'].clip(lower=age_lower, upper=age_upper).sort_values()\n",
    "print(sorted_ages)\n",
    "def min_age():\n",
    "    clipped_ages = adult['Age'].clip(lower=0, upper=100)\n",
    "    return clipped_ages.min()\n",
    "\n",
    "def ls_min():\n",
    "    return max(sorted_ages.iloc[0] - age_lower, sorted_ages.iloc[1] - sorted_ages.iloc[0])\n",
    "\n",
    "print('Actual minimum age:', min_age())\n",
    "print('Local sensitivity of the minimum:', ls_min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement `ls_min_at_distance`, an upper bound on the local sensitivity of the `min_age` query at distance $k$, and `dist_to_high_ls_min`, an upper bound on the distance from the true dataset to one with local sensitivity greater than or equal to $s_p$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ls_min_at_distance(k):\n",
    "    if k == 0:\n",
    "        return 0\n",
    "    return sorted_ages.iloc[k]\n",
    "\n",
    "def dist_to_high_ls(f, s_p):\n",
    "    dist = 0\n",
    "    while f(dist) < s_p:\n",
    "        dist += 1\n",
    "\n",
    "    return dist\n",
    "\n",
    "def dist_to_high_ls_min(s_p):\n",
    "    return dist_to_high_ls(ls_min_at_distance, s_p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TEST CASE\n",
    "print(dist_to_high_ls_min(18))\n",
    "assert dist_to_high_ls_min(18) == 395\n",
    "assert dist_to_high_ls_min(20) == 1657\n",
    "assert dist_to_high_ls_min(25) == 5570\n",
    "assert dist_to_high_ls_min(30) == 9711"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2 (10 points)\n",
    "\n",
    "Implement `ptr_min`, which should use the propose-test-release framework to calculate a differentially private estimate of the minimum age. If the test fails, return `None`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ptr_min(s_p, epsilon, delta):\n",
    "\n",
    "    k = dist_to_high_ls_min(s_p)\n",
    "\n",
    "    noisy_distance = laplace_mech(k, 1, epsilon)\n",
    "    threshold = np.log(2/delta)/(2*epsilon)\n",
    "\n",
    "    if noisy_distance >= threshold:\n",
    "        return laplace_mech(min(sorted_ages), s_p, epsilon)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# proposed sensitivity: 0.05\n",
    "# epsilon, delta = (1.0, 10^-5)\n",
    "ptr_min(20, 1.0, 1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TEST CASE\n",
    "true_min = min_age()\n",
    "trials = [ptr_min(20, 0.1, 1e-5) for _ in range(20)]\n",
    "errors = [pct_error(true_min, t) for t in trials]\n",
    "print(np.mean(errors))\n",
    "assert np.mean(errors) < 2000\n",
    "assert np.mean(errors) > 500\n",
    "\n",
    "assert ptr_min(0.0001, 0.1, 1e-5) == None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 3 (5 points)\n",
    "\n",
    "In 2-5 sentences, answer the following:\n",
    "\n",
    "- Can `ptr_mean` give a useful answer for the minimum age?\n",
    "- If so, what is a good proposed sensitivity $s_p$ for the analyst to use? If not, why not?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No, it gives us an error of about 1000%, which might as well be random noise, (given 1000% of 17 is 170, well outside the range of real ages).\n",
    "This data is too locally sensitive over the operation of min for it to be obscured practically in this way.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 4 (10 points)\n",
    "\n",
    "Consider the `median_age` function, which calculates the *median* age (this version truncates if the length of the dataset is even), and the `ls_median` function, an upper bound on the local sensitivity of the median query:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Cache the sorted ages, because we will use them a lot.\n",
    "sorted_ages = adult['Age'].clip(lower=0, upper=100).sort_values()\n",
    "\n",
    "def median_age():\n",
    "    idx = int(len(adult)/2)\n",
    "    return sorted_ages.iloc[idx]\n",
    "\n",
    "print('Median age:', median_age())\n",
    "\n",
    "def ls_median():\n",
    "    idx = int(len(adult)/2)\n",
    "    return max(sorted_ages.iloc[idx] - sorted_ages.iloc[idx-1],\n",
    "               sorted_ages.iloc[idx+1] - sorted_ages.iloc[idx])\n",
    "\n",
    "print('Local sensitivity of the median:', ls_median())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the local sensitivity of the median is 0. Implement the functions `ls_median_at_distance`, which calculates the local sensitivity at distance $k$ of the median query above, and the corresponding `dist_to_high_ls_median`.\n",
    "\n",
    "*Hint*: note that the ages are clipped. Think about the worst-case scenario of adding or removing $k$ rows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ls_median_at_distance(k):\n",
    "    idx = int(len(adult)/2)\n",
    "    return max(sorted_ages.iloc[idx] - sorted_ages.iloc[idx-k],\n",
    "               sorted_ages.iloc[idx+k] - sorted_ages.iloc[idx])\n",
    "\n",
    "def dist_to_high_ls_median(s_p):\n",
    "    return dist_to_high_ls(ls_median_at_distance, s_p)\n",
    "\n",
    "ls_median_at_distance(10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert ls_median_at_distance(500) == 1\n",
    "assert ls_median_at_distance(5000) == 6\n",
    "assert ls_median_at_distance(10000) == 14\n",
    "assert ls_median_at_distance(15000) == 28"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 5 (10 points)\n",
    "\n",
    "Use the propose-test-release framework, plus `dist_to_high_ls_median`, to answer the median query with differential privacy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ptr_median(s_p, epsilon, delta):\n",
    "    k = dist_to_high_ls_median(s_p)\n",
    "\n",
    "    noisy_distance = laplace_mech(k, 1, epsilon)\n",
    "    threshold = np.log(2/delta)/(2*epsilon)\n",
    "\n",
    "    if noisy_distance >= threshold:\n",
    "        return laplace_mech(median_age(), s_p, epsilon)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "ptr_median(0.01, 1.0, 1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TEST CASE\n",
    "true_median = median_age()\n",
    "trials = [ptr_median(0.05, 0.1, 1e-5) for _ in range(20)]\n",
    "errors = [pct_error(true_median, t) for t in trials]\n",
    "print(np.mean(errors))\n",
    "assert np.mean(errors) < 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 6 (10 points)\n",
    "\n",
    "In 2-5 sentences, answer the following:\n",
    "\n",
    "- At roughly what distance does the local sensitivity of the median become non-zero?\n",
    "- For what proposed sensitivity does `ptr_median` fail the test (i.e. return `None`)?\n",
    "- What does this mean for the amount of noise required to release the median with differential privacy?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([ls_median_at_distance(i) for i in range(500)])\n",
    "i = 0.05\n",
    "median = 0\n",
    "while median is not None:\n",
    "    median = ptr_median(i, 0.1, 1e-5)\n",
    "    i -= 0.00001\n",
    "print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "About 400.\n",
    "I cannot find a positive sensitivity which returns None.\n",
    "Because the data is very insensitive to change, we do not need to add nearly as much noise to retain differential privacy, because adding or removing people will have a much smaller effect."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 7 (20 points)\n",
    "\n",
    "Use the sample-and-aggregate framework to release the minimum age in the adult dataset. Reference [Chapter 7](https://uvm-plaid.github.io/programming-dp/notebooks/ch7.html)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def f(chunk):\n",
    "    return chunk.min()\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "random_ages = sorted_ages.copy()\n",
    "shuffle(random_ages)\n",
    "\n",
    "def saa_min_age(k, epsilon):\n",
    "    # Calculate the number of rows in each chunk\n",
    "\n",
    "    chunk_size = int(np.ceil(random_ages.shape[0] / k))\n",
    "\n",
    "    # Step 1: split into chunks\n",
    "    xs      = [random_ages[i:i+chunk_size] for i in range(0, random_ages.shape[0],chunk_size)]\n",
    "\n",
    "    # Step 2: run f on each x_i and clip its output\n",
    "    answers = [f(x_i) for x_i in xs]\n",
    "\n",
    "    u = 100\n",
    "    l = 0\n",
    "    clipped_answers = np.clip(answers, l, u)\n",
    "\n",
    "    # Step 3: take the noisy min of the answers\n",
    "    noisy_mean = laplace_mech(np.mean(clipped_answers), 100/k, epsilon)\n",
    "    return noisy_mean\n",
    "\n",
    "saa_min_age(500, 1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TEST CASE\n",
    "true_min = adult['Age'].min()\n",
    "trials = [saa_min_age(500, 1.0) for _ in range(20)]\n",
    "errors = [pct_error(true_min, t) for t in trials]\n",
    "print(np.mean(errors))\n",
    "assert np.mean(errors) > 0\n",
    "assert np.mean(errors) < 10\n",
    "\n",
    "plt.ylim([-1, 25])\n",
    "plt.plot([pct_error(true_min, saa_min_age(i, 1.0)) for i in range(1,1000)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 8 (10 points)\n",
    "\n",
    "In 5-6 sentences, answer the following:\n",
    "\n",
    "- What clipping values did you choose for clipping the query outputs on each chunk? How did you pick them?\n",
    "- Is 500 a good value for the number of chunks $k$? How does making $k$ larger or smaller change the results?\n",
    "- How does the sample-and-aggregate approach compare to propose-test-release or global sensitivity for the minimum?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "100 and 0 seem to work just fine for clipping, and i believe the ages were already clipped to this, and further clipping could introduce unnecessary human bias into the sensitivity.\n",
    "\n",
    "When k is very small, the error is large. When k is very large, it climbs back up. For this dataset, it bottoms out around 250. We can see that 500 is a decent choice but could be a little better.\n",
    "\n",
    "Much lower error in this particular instance, because there are a large number of 17 year olds in the dataset, so by splitting into chunks, we take advantage of the fact that no individual can be easily identified by the fact that they are 17, and in doing so distribute the burden of privacy cost.\n",
    "\n",
    "\n",
    "Sorry for the lateness, I've had an emotionally intense couple of weeks.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 8 (10 points)\n",
    "\n",
    "In 5-6 sentences, answer the following:\n",
    "\n",
    "- What clipping values did you choose for clipping the query outputs on each chunk? How did you pick them?\n",
    "- Is 500 a good value for the number of chunks $k$? How does making $k$ larger or smaller change the results?\n",
    "- How does the sample-and-aggregate approach compare to propose-test-release or global sensitivity for the minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "509f0c56c158e6f504dd5091e1050e7b",
     "grade": true,
     "grade_id": "cell-4bbbc2dba075a1e5",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "100 and 0 seem to work just fine for clipping, and i believe the ages were already clipped to this, and further clipping could introduce unnecessary human bias into the sensitivity.\n",
    "\n",
    "When k is very small, the error is large. When k is very large, it climbs back up. For this dataset, it bottoms out around 250. We can see that 500 is a decent choice but could be a little better.\n",
    "\n",
    "Much lower error in this particular instance, because there are a large number of 17 year olds in the dataset, so by splitting into chunks, we take advantage of the fact that no individual can be easily identified by the fact that they are 17, and in doing so distribute the burden of privacy cost.\n",
    "\n",
    "\n",
    "Sorry for the lateness, I've had an emotionally intense couple of weeks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}